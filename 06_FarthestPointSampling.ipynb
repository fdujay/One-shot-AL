{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "from sklearn.preprocessing import scale\n",
    "from modAL.density import information_density\n",
    "np.random.seed(42)\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering: Counter({3: 244, 5: 233, 2: 220, 4: 218, 7: 216, 9: 211, 8: 207, 0: 202, 6: 165, 1: 159})\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ft:挑选后的特征\n",
    "t:特征对应金标准\n",
    "name:样本名称\n",
    "c: 特征对应簇\n",
    "# '''\n",
    "df1 = pd.read_csv('../FeatureBank/ISIC2018_NPID.csv',header = None)\n",
    "# print(df1.shape)\n",
    "ft = df1.values #特征样本/256维\n",
    "# ft = scale(ft) #进行归一化\n",
    "\n",
    "df2 = pd.read_csv('../2048/train.txt',sep = ' ',header = None)#一般不修\n",
    "name = df2.iloc[:,0].values #图片位置名称\n",
    "# t = df2.iloc[:,1].values #金标准0/1\n",
    "# print('Original dataset:', Counter(t))\n",
    "\n",
    "df3 = pd.read_csv('../kmeans/ISIC18_NPID_kmeans++.csv',header = 0)\n",
    "c = df3.iloc[:,1].values\n",
    "print('Clustering:', Counter(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "ratio=0.25\n",
    "total_num = len(c)\n",
    "sel_num= math.ceil(ratio*total_num/n_clusters)\n",
    "sel_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def farthest_point_sample(xyz, npoint): \n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "\n",
    "#     xyz = xyz.transpose(2,1)\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    \n",
    "    centroids = torch.zeros(B, npoint, dtype=torch.long).to(device)     # 采样点矩阵（B, npoint）\n",
    "    distance = torch.ones(B, N).to(device) * 1e10                       # 采样点到所有点距离（B, N）\n",
    "\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)        # batch_size 数组\n",
    "\n",
    "    #farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)  # 初始时随机选择一点\n",
    "    \n",
    "    barycenter = torch.sum((xyz), 1)                                    #计算重心坐标 及 距离重心最远的点\n",
    "    barycenter = barycenter/xyz.shape[1]\n",
    "    barycenter = barycenter.view(B, 1, C)\n",
    "\n",
    "    dist = torch.sum((xyz - barycenter) ** 2, -1)\n",
    "    farthest = torch.max(dist,1)[1]                                     #将距离重心最远的点作为第一个点\n",
    "\n",
    "    for i in range(npoint):\n",
    "#         print(\"-------------------------------------------------------\")\n",
    "#         print(\"The %d farthest pts %s \" % (i, farthest))\n",
    "        centroids[:, i] = farthest                                      # 更新第i个最远点\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, C)        # 取出这个最远点的xyz坐标\n",
    "        dist = torch.sum((xyz - centroid) ** 2, -1)                     # 计算点集中的所有点到这个最远点的欧式距离\n",
    "#         print(\"dist    : \", dist)\n",
    "        mask = dist < distance\n",
    "#         print(\"mask %i : %s\" % (i,mask))\n",
    "        distance[mask] = dist[mask]                                     # 更新distance，记录样本中每个点距离所有已出现的采样点的最小距离\n",
    "#         print(\"distance: \", distance)\n",
    "\n",
    "        farthest = torch.max(distance, -1)[1]                           # 返回最远点索引\n",
    " \n",
    "    return centroids\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     sim_data = Variable(torch.rand(2,10000,128))\n",
    "#     print(sim_data)\n",
    "\n",
    "#     centroids = farthest_point_sample(sim_data, 100)\n",
    "    \n",
    "#     print(\"Sampled pts: \", centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset: Counter({0: 52, 1: 52, 2: 52, 3: 52, 4: 52, 5: 52, 6: 52, 7: 52, 8: 52, 9: 52})\n",
      "520\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Maxcover\n",
    "'''\n",
    "import math\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "index=[] #index[i]表示第i个聚类对应的index列表\n",
    "ft_part=[] #ft_part[i]表示第i个聚类对应的特征向量列表\n",
    "new_index=[] #新列表\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    index.append(np.where(c == i)[0])\n",
    "    ft_part.append(torch.Tensor([ft[index[i]]]))\n",
    "\n",
    "for i in range(n_clusters):\n",
    "#     #注意：比例\n",
    "# #     clf = IsolationForest(contamination=ratio,n_jobs=20).fit(ft_part[i])\n",
    "    \n",
    "# #     novel_pred = clf.predict(ft_part[i])\n",
    "# #     li = np.where(novel_pred==-1)\n",
    "    \n",
    "# #     novel_scores = clf.score_samples(ft_part[i])\n",
    "# #     li = np.argsort(novel_scores)[:sel_num]\n",
    "\n",
    "\n",
    "\n",
    "    centroids = farthest_point_sample(ft_part[i], sel_num).numpy().flatten()\n",
    "#     print(\"Sampled pts: \", centroids)\n",
    "    new_index.extend(index[i][centroids])\n",
    "    \n",
    "ft_sel = ft[new_index]\n",
    "# t_sel = t[new_index]\n",
    "c_sel = c[new_index]\n",
    "name_sel = name[new_index]\n",
    "\n",
    "from collections import Counter \n",
    "print('Resampled dataset:', Counter(c_sel))\n",
    "print(len(c_sel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "注意保存csv的后缀\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "注意保存csv的后缀\n",
    "'''\n",
    "print(__doc__)\n",
    "dataframe1 = pd.DataFrame({'image_name':name_sel})\n",
    "dataframe1.to_csv(\"../txt/NPID/ISIC_FPS25.txt\",index=False,sep=' ',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
